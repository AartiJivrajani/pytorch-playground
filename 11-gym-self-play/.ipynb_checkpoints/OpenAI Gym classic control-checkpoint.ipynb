{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "from random import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "class PolicyShallowNN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(PolicyShallowNN, self).__init__()\n",
    "        self.fc = nn.Linear(input_length, output_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "class PolicyDeepNN(nn.Module):\n",
    "    def __init__(self, input_length, output_length):\n",
    "        super(PolicyDeepNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_length, 64)\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.fc3 = nn.Linear(128, output_length)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "def mountaincar_reward_func(_, states):\n",
    "    return max([state[0] for state in states])\n",
    "    \n",
    "fields = [\n",
    "    'name',\n",
    "    'model_class',\n",
    "    # the rest are optional, in case it's needed for the env\n",
    "    'reward_func',\n",
    "    'output_quantization_levels',\n",
    "    'output_range',\n",
    "    'reward_threshold',\n",
    "    'learning_rate',\n",
    "]\n",
    "EnvDescription = collections.namedtuple('EnvDescription', fields, defaults=[None] * len(fields))    \n",
    "\n",
    "env_descriptions = [\n",
    "    EnvDescription(name='CartPole-v1',\n",
    "                   model_class=PolicyShallowNN,\n",
    "                  ),\n",
    "    EnvDescription(name='Acrobot-v1',\n",
    "                   model_class=PolicyShallowNN,\n",
    "                  ),\n",
    "    EnvDescription(name='Pendulum-v0',\n",
    "                   model_class=PolicyDeepNN,\n",
    "                   output_quantization_levels=41,\n",
    "                   output_range=[-2, 2],\n",
    "                   reward_threshold=-150,\n",
    "                  ),\n",
    "    EnvDescription(name='MountainCar-v0',\n",
    "                   model_class=PolicyDeepNN,\n",
    "                   reward_func=mountaincar_reward_func,\n",
    "                   reward_threshold=0.51,\n",
    "                   learning_rate=0.001,\n",
    "                  ),\n",
    "]    \n",
    "\n",
    "def reward_threshold(env, env_desc):\n",
    "    if env_desc.reward_threshold is not None:\n",
    "        return env_desc.reward_threshold\n",
    "    else:\n",
    "        return env.spec.reward_threshold\n",
    "\n",
    "def action_vector_from_policy(model, state, eps_non_greedy):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    x = model(state)\n",
    "    m = Categorical(x)\n",
    "    if random() < eps_non_greedy:\n",
    "        # off policy\n",
    "        action = int(len(x[0]) * random())\n",
    "        return action, x[0]\n",
    "    else:\n",
    "        # on policy\n",
    "        action = m.sample()\n",
    "        return action.item(), x[0]\n",
    "\n",
    "def select_action_from_policy(model, state, eps_non_greedy):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(state)\n",
    "    m = Categorical(probs)\n",
    "    if random() < eps_non_greedy:\n",
    "        # off policy\n",
    "        action = int(len(probs[0]) * random())\n",
    "        return action, m.logits[0][action]\n",
    "    else:\n",
    "        # on policy\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)[0]\n",
    "\n",
    "def select_action_from_policy_best(model, state, eps_non_greedy):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = model(state)\n",
    "    m = Categorical(probs)\n",
    "    if random() < eps_non_greedy:\n",
    "        # off policy\n",
    "        action = int(len(probs[0]) * random())\n",
    "        return action, m.logits[0][action]\n",
    "    else:\n",
    "        action = torch.argmax(probs[0]).item()\n",
    "        return action, m.logits[0][action]\n",
    "    \n",
    "def play_one_game(env_desc, env, model, eps_non_greedy, policy_func=select_action_from_policy):\n",
    "    state = env.reset()\n",
    "    rewards, probs, actions, states = [], [], [], []\n",
    "    for t in range(env._max_episode_steps):\n",
    "        action, prob = policy_func(model, state, eps_non_greedy)\n",
    "        actions.append(action)\n",
    "        probs.append(prob)\n",
    "        if env_desc.output_quantization_levels is not None:\n",
    "            action = env_desc.output_range[0] + action * float(env_desc.output_range[1] - env_desc.output_range[0]) / (env_desc.output_quantization_levels-1)\n",
    "            action = np.array([action])\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    if env_desc.reward_func is not None:\n",
    "        episode_reward = env_desc.reward_func(rewards, states)\n",
    "    else:\n",
    "        episode_reward = sum(rewards)\n",
    "    return episode_reward, probs, actions\n",
    "    \n",
    "def play_games(env_desc, env, model, num_episodes, eps_non_greedy):\n",
    "    games = []\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, probs, actions = play_one_game(env_desc, env, model, eps_non_greedy)\n",
    "        games.append((episode_reward, probs, actions))\n",
    "    return games\n",
    "\n",
    "def in_out_length(env):\n",
    "    input_length = env.observation_space.shape[0]\n",
    "    if isinstance(env.action_space, gym.spaces.box.Box):\n",
    "        output_length = env.action_space.shape[0]\n",
    "    elif isinstance(env.action_space, gym.spaces.discrete.Discrete):\n",
    "        output_length = env.action_space.n\n",
    "    return input_length, output_length\n",
    "\n",
    "def compute_loss(probs, rewards):\n",
    "    loss = 0\n",
    "    for p, r in zip(probs, rewards):\n",
    "        loss += p * r\n",
    "    loss *= -1\n",
    "    return loss\n",
    "\n",
    "def mean(li):\n",
    "    return sum(li)/len(li)\n",
    "\n",
    "def train_solve_self_play(env_desc, model=None, num_epochs=5000, num_episodes=100, eps_non_greedy=0.0):\n",
    "    print('Doing %s' % env_desc.name)\n",
    "    env = gym.make(env_desc.name)\n",
    "    input_length, output_length = in_out_length(env)\n",
    "    if env_desc.output_quantization_levels is not None:\n",
    "        output_length *= env_desc.output_quantization_levels\n",
    "    if model is None:\n",
    "        model = env_desc.model_class(input_length, output_length)\n",
    "    if env_desc.learning_rate != None:\n",
    "        lr = env_desc.learning_rate\n",
    "    else:\n",
    "        lr = 0.01\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        games = play_games(env_desc, env, model, num_episodes, eps_non_greedy)\n",
    "        games.sort(key=lambda x: x[0]) # sorts by first key, the reward\n",
    "        losses = games[:int(num_episodes/2)]\n",
    "        wins = games[int(num_episodes/2):]\n",
    "        sum_loss = 0\n",
    "        for game in losses:\n",
    "            probs = game[1]\n",
    "            sum_loss += compute_loss(probs, [-1] * len(probs))\n",
    "        for game in wins:\n",
    "            probs = game[1]\n",
    "            sum_loss += compute_loss(probs, [+1] * len(probs))\n",
    "        optimizer.zero_grad()\n",
    "        sum_loss.backward()\n",
    "        optimizer.step()\n",
    "        evaluation_rewards = []\n",
    "        for _ in range(10):\n",
    "            reward, _, _ = play_one_game(env_desc, env, model, eps_non_greedy=0.0, policy_func=select_action_from_policy_best)\n",
    "            evaluation_rewards.append(reward)\n",
    "        mean_reward = mean(evaluation_rewards)\n",
    "        print('%d: min=%.2f median=%.2f max=%.2f eval=%.2f' % (epoch, games[0][0], games[int(num_episodes/2)][0], games[-1][0], mean_reward))\n",
    "        if mean_reward >= reward_threshold(env, env_desc):\n",
    "            print('Solved!')\n",
    "            return model\n",
    "    print('Failed!')\n",
    "    return model\n",
    "\n",
    "models = []\n",
    "for env_desc in env_descriptions:\n",
    "    model = train_solve_self_play(env_desc)\n",
    "    models.append(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
